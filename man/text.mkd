##### Keywords

* LDA topic-probability distribution per document (p(z|d))

##### Hadoop on Mac OS X - [ [Ubuntu](http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/) | [Mac OS X](http://wiki.apache.org/hadoop/Running_Hadoop_On_OS_X_10.5_64-bit_\(Single-Node_Cluster\)) ]

* `brew install hadoop`
* Setup [passphraseless ssh](http://hadoop.apache.org/docs/stable/single_node_setup.html#Setup+passphraseless)
  * `ssh-keygen -t rsa`, and then `cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys`
* Setup `libexec/conf/hadoop-env.sh` and [pseudo-distributed operation](http://hadoop.apache.org/docs/stable/single_node_setup.html#PseudoDistributed)
* Bring up the [name node](http://localhost:50070/) and [job tracker](http://localhost:50030/)
* Upload & Downloads local files to DFS by [`hadoop dfs -put`](http://hadoop.apache.org/docs/stable/file_system_shell.html#put), [`hadoop dfs -get`](http://hadoop.apache.org/docs/stable/file_system_shell.html#get), and then [`hadoop dfs -ls`](http://hadoop.apache.org/docs/stable/file_system_shell.html#ls)

```bash
curl -o '/usr/local/Cellar/hadoop/1.1.2/libexec/conf/hadoop-env.sh' \
  -kL 'https://raw.github.com/henry4j/-/master/bin/hadoop-env.sh'
curl -o '/usr/local/Cellar/hadoop/1.1.2/libexec/conf/#1-site.xml' \
  -kL 'https://raw.github.com/henry4j/-/master/bin/{core,hdfs,mapred}-site.xml'
rm -rf /usr/local/tmp/hadoop-$USER && mkdir -p /usr/local/tmp/hadoop-$USER
stop-all.sh && hadoop namenode -format
```

```bash
start-all.sh # starts up hadoop daemons: name, data, 2nd name nodes, and job & task tracker.
ps aux | grep hadoop | grep -o 'org.apache.[^ ]\+$' # sees 5 lines for hadoop daemons.
hadoop jar /usr/local/Cellar/hadoop/1.1.2/libexec/hadoop-examples-1.1.2.jar pi 10 100 # same as ruby -e 'p Math::PI'
stop-all.sh # stops hadoop daemons.
```

```bash
cat <<EOF >> /usr/local/bin/restart-all
#!/bin/bash
stop-all.sh
start-all.sh
hadoop dfsadmin -safemode leave
EOF
chmod +x /usr/local/bin/restart-all
```

##### [Mahout 0.8](http://search.maven.org/#search%7Cga%7C1%7Cmahout)

```bah
[ ! -d /workspace ] && sudo mkdir /workspace # make one unless there is.
curl -o $HOME/Downloads/mahout-distribution-0.8.zip \
  -kL http://www.motorlogy.com/apache/mahout/0.8/mahout-distribution-0.8.zip
unzip -o $HOME/Downloads/mahout-distribution-0.8.zip -d /workspace/
ln -s /workspace/mahout-distribution-0.8 /workspace/mahout
```

##### Mahout (optionally, the last stable build)

* Source: `svn checkout http://svn.apache.org/repos/asf/mahout/trunk /workspace/mahout-trunk`

```bash
ruby -e '
  output = %q(/workspace/mahout)
  source = %q(https://builds.apache.org/job/Mahout-Quality/lastStableBuild/artifact/trunk)
  Dir.glob(%Q(#{output}/*0.8*.jar)).each { |e| File.rename(e, %Q(#{e}.bak)) }
  %w(
    core/target/mahout-core-0.9-SNAPSHOT-job.jar core/target/mahout-core-0.9-SNAPSHOT.jar
    examples/target/mahout-examples-0.9-SNAPSHOT-job.jar examples/target/mahout-examples-0.9-SNAPSHOT.jar
    integration/target/mahout-integration-0.9-SNAPSHOT.jar math/target/mahout-math-0.9-SNAPSHOT.jar
  ).each { |e|
    system %Q(curl -o #{output}/#{File.basename(e)} -kL #{source}/#{e})
  }
'
```

```bash
ruby -e '
  output = %q(/workspace/mahout)
  Dir.glob(%Q(#{output}/*0.9*.jar)).each { |e| File.delete(e) }
  Dir.glob(%Q(#{output}/*0.8*.jar.bak)).each { |e| File.rename(e, %Q(#{e[0..-5]})) }
'
```

##### Mahout LDA CVB Demo 


```bash
# Set up alias and work dir.
export MAHOUT="/workspace/mahout/bin/mahout" # same as mahout
export HADOOP="/usr/local/bin/hadoop" # same as hadoop
export WORK_DIR=/tmp/mahout-work-${USER}
[ ! -d $WORK_DIR ] && mkdir -p ${WORK_DIR}
```

###### Prepare Document Collection <sup>from Lucene Batchmark Program</sup> <sub>to Hadoop-native sequence files</sub>

```bash
if [ ! -e ${WORK_DIR}/comm-text-ext ]; then
    if [ ! -e ${WORK_DIR}/comm-text-sgm ]; then
        if [ ! -f $HOME/Downloads/reuters21578.tar.gz ]; then
            echo "Downloading Reuters-21578"
            curl http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz -o $HOME/Downloads/reuters21578.tar.gz
        fi
        mkdir -p ${WORK_DIR}/comm-text-sgm
        echo "Extracting..."
        tar xzf $HOME/Downloads/reuters21578.tar.gz -C ${WORK_DIR}/reuters-sgm
    fi

    # Safe to ignore WARN: driver.MahoutDriver:
    #   [No org.apache.lucene.benchmark.utils.ExtractReuters.props found on classpath]
    #   (http://lucene.apache.org/core/4_4_0/benchmark/org/apache/lucene/benchmark/utils/ExtractReuters.html)
    echo "Extracting Reuters"
    $MAHOUT org.apache.lucene.benchmark.utils.ExtractReuters ${WORK_DIR}/reuters-sgm ${WORK_DIR}/comm-text-ext
    # or, prep-comm-text $HOME/Downloads/comm-text.csv -d ${WORK_DIR}/comm-text-ext # 14,794 documents/58MB

    echo "Copying Reuters data to Hadoop"
    $HADOOP dfs -rmr ${WORK_DIR}/comm-text-ext || true
    $HADOOP dfs -put ${WORK_DIR}/comm-text-ext ${WORK_DIR}/comm-text-ext
    
    echo "Converting to Sequence Files"
    $MAHOUT seqdirectory -i ${WORK_DIR}/comm-text-ext -o ${WORK_DIR}/comm-text-seq -ow -c UTF-8 -chunk 5
fi
```

###### Topic Modeling w/o repetitive tuning cycles

* Let's do away with tuning required by seq2sparse and cvb for the better results.

```bash
hadoop dfs -rmr ${WORK_DIR}/comm-text-unigram*
$MAHOUT seq2sparse \
  -i ${WORK_DIR}/comm-text-seq/ \
  -o ${WORK_DIR}/comm-text-unigram -ow \
  -a org.apache.mahout.text.MailArchivesClusteringAnalyzer \
  --namedVector -s 80 -x 70 # excludes terms of 80- DF times & 70+ DF%; took 184,207 ms

hadoop dfs -rmr comm-text-unigram/*-matrix
$MAHOUT rowid \
  -i ${WORK_DIR}/comm-text-unigram/tfidf-vectors \
  -o ${WORK_DIR}/comm-text-unigram/tfidf-matrix
$MAHOUT rowid \
  -i ${WORK_DIR}/comm-text-unigram/tf-vectors \
  -o ${WORK_DIR}/comm-text-unigram/tf-matrix # 14774 x 441; 14774 x 398

rm -rf ${WORK_DIR}/comm-text-unigram-*
hadoop dfs -rmr ${WORK_DIR}/comm-text-unigram/model
$MAHOUT cvb \
  -dict ${WORK_DIR}/comm-text-unigram/dictionary.file-0 \
  -i  ${WORK_DIR}/comm-text-unigram/tfidf-matrix/matrix \
  -o  ${WORK_DIR}/comm-text-unigram/model -ow \
  -mt ${WORK_DIR}/comm-text-unigram/modeling \
  -dt ${WORK_DIR}/comm-text-unigram/topics \
  -k 20 -x 35 -cd 6e-4 -block 2 -tf 0.25 -seed 777
```

```bash
hadoop dfs -rmr ${WORK_DIR}/comm-text-bigram*
$MAHOUT seq2sparse \
  -i ${WORK_DIR}/comm-text-seq/ \
  -o ${WORK_DIR}/comm-text-bigram -ow \
  -a org.apache.mahout.text.MailArchivesClusteringAnalyzer \
  --namedVector -s 80 -x 70 -ng 2 # excludes terms of 80- DF times & 85+ DF%; took 184,207 ms

hadoop dfs -rmr comm-text-bigram/*-matrix
$MAHOUT rowid \
  -i ${WORK_DIR}/comm-text-bigram/tfidf-vectors \
  -o ${WORK_DIR}/comm-text-bigram/tfidf-matrix
$MAHOUT rowid \
  -i ${WORK_DIR}/comm-text-bigram/tf-vectors \
  -o ${WORK_DIR}/comm-text-bigram/tf-matrix # 14774 x 699; 14774 x 610

rm -rf ${WORK_DIR}/comm-text-bigram-*
hadoop dfs -rmr ${WORK_DIR}/comm-text-bigram/model
$MAHOUT cvb \
  -dict ${WORK_DIR}/comm-text-bigram/dictionary.file-0 \
  -i  ${WORK_DIR}/comm-text-bigram/tfidf-matrix/matrix \
  -o  ${WORK_DIR}/comm-text-bigram/model -ow \
  -mt ${WORK_DIR}/comm-text-bigram/modeling \
  -dt ${WORK_DIR}/comm-text-bigram/topics \
  -k 20 -x 35 -cd 6e-4 -block 2 -tf 0.25 -seed 777
```

```bash
curl -o '/usr/local/bin/pp-w,z' -kL 'https://raw.github.com/henry4j/-/master/bin/pp-w,z'
curl -o '/usr/local/bin/pp-z,d' -kL 'https://raw.github.com/henry4j/-/master/bin/pp-z,d'
chmod +x /usr/local/bin/pp-*
```

```bash
mahout seqdumper -i $WORK_DIR/comm-text-unigram/tokenized-documents -o unigram-token-d.txt
mahout seqdumper -i $WORK_DIR/comm-text-bigram/tokenized-documents  -o bigram-token-d.txt

mahout vectordump \
  -i $WORK_DIR/comm-text-unigram-model -o unigram-w,z.txt \
  -p true -sort $WORK_DIR/comm-text-unigram-model -vs 25 \
  -d $WORK_DIR/comm-text-unigram/dictionary.file-0 -dt sequencefile
mahout vectordump -i $WORK_DIR/comm-text-unigram-topics -o unigram-z,d.txt

mahout vectordump \
  -i $WORK_DIR/comm-text-bigram-model -o bigram-w,z.txt \
  -p true -sort $WORK_DIR/comm-text-bigram-model -vs 25 \
  -d $WORK_DIR/comm-text-bigram/dictionary.file-0 -dt sequencefile
mahout vectordump -i $WORK_DIR/comm-text-bigram-topics -o bigram-z,d.txt

pp-w,z unigram-w,z.txt
pp-z,d unigram-z,d.txt -n 10
head -11 unigram-token-d.txt

pp-w,z bigram-w,z.txt
pp-z,d bigram-z,d.txt -n 10
head -11 bigram-token-d.txt
```

###### Noises

* amazon.com, help, problem(s), question(s), issue(s)

###### Deep Dive into Output

* `comm-text-vector` by `seq2sparse`
  * tokenized-documents -- tokens (string tuples) keyed by docs.
     * `mahout seqdumper -i $WORK_DIR/comm-text-bigram/tokenized-documents -o token_vectors.txt`
     * e.g. key: /reut2-010.sgm-92.txt: value: [26, mar, 1987, 14, 39, 20.76, dexter, dex, units, sets, license, toyota, unit, dexter, corp's, hysol, aerospace, industrial, products, division, said, agreed, license, its, engineering, adhesives, toyota, motor, co's, toyoda, gosei, unit, two, units, jointly, develop, line, structural, adhesive, application, techniques, automotive, certain, other, industries, reuter, 3]
  * dictionary.file-0 -- integer IDs keyed by term.
     * `mahout seqdumper -i $WORK_DIR/comm-text-bigram/dictionary.file-0 -o dictionary.txt` # returns 41807 terms.
     * e.g. key: `0`: value: 0, key: `amazon`: value: 13707
  * wordcount
     * `mahout seqdumper -i $WORK_DIR/comm-text-bigram/wordcount/ngrams -o bigrams.txt`
     * key: `amazon`: value: 6 # grep -w amazon token_vectors.txt | grep -o amazon | wc -l
  * frequency.file-0 -- frequencies keyed by term ID.
     * `mahout seqdumper -i $WORK_DIR/comm-text-bigram/frequency.file-0 -o frequency.txt`
     * e.g. key: 0: value: 43; key: 13707: value: 4 -- term `0` occurs 43 times; term `amazon` occurs in 4 documents.
  * df-count
     * `mahout seqdumper -i $WORK_DIR/comm-text-bigram/frequency.file-0 -q | grep 13707` # shows `amazon` occurs in 4 documents.
  * tf-vectors
     * `mahout vectordump -i $WORK_DIR/comm-text-bigram/tf-vectors/part-r-00000 -o tf-vectors.txt` # returns 21,578 docs.
     * `grep -o '13707:[[:digit:]]\+' tf-vectors.txt | paste -s -d, -` # 13707:1,13707:3,13707:1,13707:1
  * tfidf-vectors
     * `mahout vectordump -i $WORK_DIR/comm-text-bigram/tfidf-vectors -o tfidf_vectors.txt` # returns 21,578 docs.
     * `grep -o '13707:[[:digit:]]\+' tfidf_vectors.txt | paste -s -d, -` # 13707:9,13707:16,13707:9,13707:9
* comm-text-matrix
  * docIndex
     * `mahout seqdumper -i $WORK_DIR/comm-text-matrix/docIndex -q | head`
     * e.g. key: 1: value: /reut2-000.sgm-1.txt; key: 21577: value: /reut2-021.sgm-99.txt
  * matrix
     * `mahout seqdumper -i $WORK_DIR/comm-text-matrix/matrix -o matrix.txt`
     * `grep -o '31707:[^,]\+' matrix.txt | paste -s -d, -` # see  
       31707:12.5864839553833,31707:8.899988174438477,31707:8.899988174438477,31707:8.899988174438477,  
       31707:15.415231704711914,31707:19.900978088378906,31707:15.415231704711914

```bash
pp-z,d doc-topic-inference.txt

{0=>"30.8%", 3=>"22.1%", 7=>"22.1%", 10=>"13.4%"}
{4=>"31.0%", 13=>"20.2%", 16=>"13.0%", 17=>"21.3%"}
{2=>"88.3%"}
{2=>"94.2%"}
{1=>"19.8%", 2=>"41.9%", 19=>"38.3%"}
{1=>"13.5%", 2=>"40.7%", 16=>"45.7%"}
{3=>"11.2%", 7=>"28.4%", 10=>"45.8%"}
{0=>"47.9%", 12=>"42.6%"}
{1=>"93.5%"}
{9=>"78.6%", 12=>"12.6%"}
```

```bash
pp-w,z term-topic-distribution.txt

 0 (/1k): {"tonnes"=>5.7, "ec"=>3.6, "export"=>2.8, "european"=>2.7, "trade"=>2.5, "sugar"=>2.4, "community"=>2.4, "wheat"=>2.3, "agriculture"=>2.2, "u.s"=>2.1}
10 (/1k): {"crop"=>1.6, "corn"=>1.3, "usda"=>1.1, "grain"=>1.1, "said"=>1.0, "chrysler"=>1.0, "land"=>0.9, "wheat"=>0.9, "weather"=>0.9, "crops"=>0.9}
 1 (/1k): {"record"=>5.0, "7"=>4.1, "dividend"=>4.1, "april"=>3.9, "apr"=>3.8, "cts"=>3.8, "div"=>3.3, "pay"=>3.3, "prior"=>3.2, "sets"=>3.0}
11 (/1k): {"economic"=>4.2, "growth"=>4.0, "economy"=>3.4, "japan"=>2.4, "he"=>2.4, "domestic"=>2.3, "japanese"=>2.1, "more"=>2.1, "inflation"=>2.1, "trade"=>2.0}
 2 (/1k): {"vs"=>18.6, "net"=>11.7, "cts"=>11.3, "shr"=>10.2, "qtr"=>8.0, "mln"=>7.9, "loss"=>7.6, "revs"=>6.7, "note"=>5.6, "profit"=>4.9}
12 (/1k): {"oil"=>2.4, "union"=>2.3, "workers"=>2.3, "strike"=>1.9, "said"=>1.8, "coffee"=>1.6, "had"=>1.4, "brazil"=>1.4, "south"=>1.4, "talks"=>1.3}
 3 (/1k): {"prices"=>4.0, "oil"=>3.1, "futures"=>2.5, "traders"=>2.1, "market"=>2.1, "higher"=>2.1, "price"=>1.8, "demand"=>1.6, "lower"=>1.6, "cents"=>1.4}
13 (/1k): {"contract"=>3.2, "said"=>2.7, "plant"=>2.4, "air"=>2.1, "unit"=>1.9, "co"=>1.9, "company"=>1.9, "corp"=>1.7, "venture"=>1.7, "electric"=>1.6}
 4 (/1k): {"stake"=>3.1, "exchange"=>2.8, "securities"=>2.6, "commission"=>2.5, "stock"=>2.2, "said"=>2.2, "group"=>2.1, "would"=>2.1, "its"=>2.0, "trading"=>2.0}
14 (/1k): {"bank"=>7.2, "banks"=>5.9, "debt"=>4.0, "loans"=>3.5, "billion"=>3.5, "loan"=>3.2, "foreign"=>2.8, "commercial"=>2.6, "interest"=>2.4, "bankers"=>2.3}
 5 (/1k): {"inc"=>3.5, "company"=>3.5, "said"=>3.4, "court"=>2.4, "corp"=>2.3, "president"=>2.3, "has"=>2.3, "its"=>2.3, "officer"=>2.1, "executive"=>2.1}
15 (/1k): {"house"=>2.7, "budget"=>2.7, "tax"=>2.5, "would"=>2.4, "committee"=>2.4, "bill"=>2.3, "congress"=>1.9, "senate"=>1.9, "plan"=>1.6, "he"=>1.6}
 6 (/1k): {"computer"=>2.7, "systems"=>2.2, "said"=>1.9, "stg"=>1.8, "system"=>1.7, "market"=>1.5, "software"=>1.4, "products"=>1.2, "england"=>1.1, "computers"=>1.1}
16 (/1k): {"pct"=>4.0, "bond"=>3.5, "issue"=>3.1, "ltd"=>3.0, "1"=>2.8, "manager"=>2.8, "lead"=>2.6, "mln"=>2.5, "coupon"=>2.4, "8"=>2.4}
 7 (/1k): {"pct"=>6.0, "rose"=>4.9, "february"=>4.7, "year"=>4.3, "from"=>3.9, "january"=>3.9, "billion"=>3.6, "1986"=>3.6, "fell"=>3.2, "compared"=>2.9}
17 (/1k): {"shares"=>8.3, "common"=>5.9, "stock"=>5.7, "dlrs"=>5.1, "inc"=>4.7, "offering"=>4.7, "said"=>4.2, "share"=>3.9, "debt"=>3.8, "corp"=>3.7}
 8 (/1k): {"dlrs"=>4.4, "quarter"=>4.4, "earnings"=>4.0, "mln"=>3.6, "1986"=>3.3, "year"=>2.9, "company"=>2.7, "expects"=>2.7, "results"=>2.5, "share"=>2.4}
18 (/1k): {"dollar"=>3.4, "west"=>2.6, "german"=>2.5, "dealers"=>2.5, "currency"=>2.5, "marks"=>2.4, "rates"=>2.4, "fed"=>2.4, "yen"=>2.0, "market"=>2.0}
 9 (/1k): {"u.s"=>2.6, "reagan"=>2.0, "he"=>1.8, "iran"=>1.8, "japanese"=>1.8, "japan"=>1.6, "trade"=>1.6, "states"=>1.5, "united"=>1.5, "officials"=>1.5}
19 (/1k): {"he"=>4.4, "we"=>3.7, "analysts"=>3.3, "i"=>2.8, "have"=>2.1, "would"=>2.0, "analyst"=>2.0, "think"=>1.8, "market"=>1.7, "could"=>1.7}
```

##### Make a doc-topic inference for a new doc by [TopicModel#trainDocTopicModel](https://builds.apache.org/job/Mahout-Quality/javadoc/org/apache/mahout/clustering/lda/cvb/TopicModel.html#trainDocTopicModel(org.apache.mahout.math.Vector, org.apache.mahout.math.Vector, org.apache.mahout.math.Matrix\))?

* [My Hello LDA! app -- to be updated w/ feedback from Jake M. (Principal SDE)](http://mahout.markmail.org/message/gjrfbjykwbjjm5gp)

```java
@Test
public void testDocumentTopicInferenceForNewDocsOverReuters() {
    val conf = new Configuration();
    val dictionary = readDictionary(new Path("/tmp/dictionary"), conf);
    assertThat(dictionary.length, equalTo(41807));

    // reads 'model' dense matrix (20 x 41K), and in 'topicSum' dense vector.
    TopicModel model = readModel(dictionary, new Path("/tmp/lda-model-splits"), conf);
    assertThat(model.getNumTopics(), equalTo(20));
    assertThat(model.getNumTerms(), equalTo(41807));

    val doc = takeOnlineDocument(conf);
    Vector docTopics = new DenseVector(new double[model.getNumTopics()]).assign(1.0 / model.getNumTopics());
    Matrix docTopicModel = new SparseRowMatrix(model.getNumTopics(), doc.size());

    for (int i = 0; i < 20 /* maxItrs */; i++) {
        model.trainDocTopicModel(doc, docTopics, docTopicModel);
        System.out.println(strip(docTopics, 0.1).toString());
    }

    // tests against doc-topic inference from training
    assertThat(Math.round(docTopics.get(0) - 31), lessThanOrEqualTo(1L));
    assertThat(Math.round(docTopics.get(3) - 22), lessThanOrEqualTo(1L));
    assertThat(Math.round(docTopics.get(7) - 22), lessThanOrEqualTo(1L));
    assertThat(Math.round(docTopics.get(10) - 13), lessThanOrEqualTo(1L));
}

private static Vector strip(Vector v, double min) {
    Vector sv = new SequentialAccessSparseVector(v.size());
    for (val e : v.all()) {
        if (e.get() > min) {
            sv.set(e.index(), e.get());
        }
    }
    return sv;
}

private static Vector takeOnlineDocument(Configuration conf) {
    // tfidf_vector represents a document in RandomAccessSparseVector.
    // TODO: make a TFIDF vector out of an online string agaist dictionary, and frequency files.
    val tfidf_vector = readTFVectorsInRange(new Path("/tmp/tfidf-vectors"), conf, 0, 1)[0].getSecond();
    assertThat(tfidf_vector.size(), equalTo(41807));
    return tfidf_vector;
}

@SneakyThrows({ IOException.class })
private static Pair<String, Vector>[] readTFVectorsInRange(Path path, Configuration conf, int offset, int length) {
    val seq = new SequenceFile.Reader(FileSystem.get(conf), path, conf);
    val documentName = new Text();
    @SuppressWarnings("unchecked")
    Pair<String, Vector>[] vectors = new Pair[length];
    VectorWritable vector = new VectorWritable();
    for (int i = 0; i < offset + length && seq.next(documentName, vector); i++) {
        if (i >= offset) {
            vectors[i - offset] = Pair.of(documentName.toString(), vector.get());
        }
    }
    return vectors;
}

@SneakyThrows({ IOException.class })
private static TopicModel readModel(String[] dictionary, Path path, Configuration conf) {
    double alpha = 0.0001; // default: doc-topic smoothing
    double eta = 0.0001; // default: term-topic smoothing
    double modelWeight = 1f;
    return new TopicModel(conf, eta, alpha, dictionary, 1, modelWeight, listModelPath(path, conf));
}

@SneakyThrows({ IOException.class })
private static Path[] listModelPath(Path path, Configuration conf) {
    if (FileSystem.get(conf).isFile(path)) {
        return new Path[] { path };
    } else {
        val statuses = FileSystem.get(conf).listStatus(path, PathFilters.partFilter());
        val modelPaths = new Path[statuses.length];
        for (int i = 0; i < statuses.length; i++) {
            modelPaths[i] = new Path(statuses[i].getPath().toUri().toString());
        }
        return modelPaths;
    }
}

@SneakyThrows({ IOException.class })
private static String[] readDictionary(Path path, Configuration conf) {
    val term = new Text();
    val id = new IntWritable();
    val reader = new SequenceFile.Reader(FileSystem.get(conf), path, conf);
    val termIds = ImmutableList.<Pair<String, Integer>>builder();
    int maxId = 0;
    while (reader.next(term, id)) {
        termIds.add(Pair.of(term.toString(), id.get()));
        maxId = max(maxId, id.get());
    }
    String[] terms = new String[maxId + 1];
    for (val termId : termIds.build()) {
        terms[termId.getSecond().intValue()] = termId.getFirst().toString();
    }
    return terms;
}
```

##### Introduction to Clustering Text -- [Mahout algorithms](https://cwiki.apache.org/confluence/display/MAHOUT/Algorithms)

* text clustering: having a text processing tool that can automatically group similar items and present the results with summarizing labels is a good way to wade through large amount of text or search results without having to read all, or even most, of the content.
* cluster is an unsupervised task w/ no human interaction, such as annotating training text, required that can automatically put related content into buckets; in some cases, it also can assign **labels** to buckets and even give **summaries** of what's in each bucket.
* carrot vs. mahout
  * merits and demerits.
* how clustering can be appied at the word- or topic-level to identify keywords, or topics in documents (called topic modeling) using LDA (latent dirichlet allocation).
* performance: how fast? and how good?
* e.g. Google News, not just good algos, but **scalability**
  * factors: title, text, and publication time; they use various clustering algorithms to identify closely related stories.
  * there is more than running clustering algorithms - grouping news content on a near-real-time basis - designed to be scale.
  * quickly cluster large # of documents, determine representative documents or labels for display, and deal with new, incoming documents.

##### Types of clustering

* clustering is also useful for many other things besides text, like grouping users or data from a series of sensors, but those are outside the scope of this book.
* clustering documents is usually an offline batch processing job, so it's often worthwhile to spend extra time to get better results.
  * descriptions for clusters are often generated by looking at the most important terms by some weighting mechanism (such as TF-IDF) in documents closest to the centroid.
  * using n-grams (or identifying phrases) may also be worth experimenting with when testing description(?) approaches.
* clustering words into topics (also called topic modeling) is an effective way to quickly find topics that are covered in a large set of documents.
  * we assume that documents often cover several different topics and that words related to a given topic often found near each other -- quickly find which words appearch near each other and what documents are attached those words; in a sense, topic modeling also does document clustering.
  * topics themselves lack names, so naming it is the job of the person generating the topics. generating topics for a collection is one more way to aid users in browsing the collection and discovering interests, while we don't even know what documents contain which topic.

<table>
  <thead>
    <tr>
      <td>Topic 0</td>
      <td>Topic 1</td>
    </th>
  </thread>
  <tbody>
    <tr>
      <td>win saturday time game know nation u more after two take over back has from texa first day man offici 2 high one sinc some sunday</td>
      <td>yesterday game work new last over more most year than two from state after been would us polic peopl team run were open five american</td>
    </tr>
  </tbody>
</table>

* choosing clustering algorithms
  * hieararchical approach that runs non-linear time vs. flat approach that runs in linear time.
  * in hard- vs. soft-membership, required to rebuild clusters with new documents or not.
  * adaptive with user feedback, required to specify the number of clusters, performance and quality.
* similarity is implemented as a measure of distance between two documents that are represented as sparse vectors in p-norm with TF-IDF weights.
  * Euclidean or Cosine distance measures are appropriate for 2-norm, while Manhattan distance measure is for 1-norm.
* labeling clustering results involves utilizing concept/topic modeling by Latent Dirichlet Allocation, or
  * picking representative documents from a cluster (randomly, near from centroid, or by membership likelihood).
  * picking good labels by important terms or phrases in a cluster, a weighted list of terms by TF-IDF, a list of phrases by n-grams.

###### Install [Eclipse Kepler (4.3)](http://www.eclipse.org/downloads/)

```bash
curl -o ~/Downloads/eclipse-standard-kepler-R-macosx-cocoa-x86_64.tar.gz -kL http://ftp.osuosl.org/pub/eclipse/technology/epp/downloads/release/kepler/R/eclipse-standard-kepler-R-macosx-cocoa-x86_64.tar.gz
tar xvf ~/Downloads/eclipse-standard-kepler-R-macosx-cocoa-x86_64.tar.gz -C /Applications/
open /Applications/eclipse/Eclipse.app # and then keep this in dock
```

* Eclipse | Help | Install New Software... | Add...
* Enter `m2e` and `http://download.eclipse.org/technology/m2e/releases/` into Add Repository | OK
* Select All | Next | Next | Accept EULA | Finish | Restart Now

##### How-To: Spawn Up a text taming app Java project

```bash
echo | mvn archetype:generate \
  -DarchetypeGroupId=org.apache.maven.archetypes \
  -DarchetypeArtifactId=maven-archetype-quickstart \
  -DarchetypeVersion=1.1 \
  -DgroupId=com.henry4j \
  -DartifactId=text \
  -Dversion=1.0-SNAPSHOT \
  -DpackageName=com.henry4j \
  -DinteractiveMode=false
```

* add dependencies to pom.xml -- see [head revision](https://github.com/henry4j/-/blob/master/sources/text/pom.xml)
  * [google-guava-14.0.1.jar](http://search.maven.org/#artifactdetails%7Ccom.google.guava%7Cguava%7C14.0.1%7Cbundle)
  * [opencsv-2.3.jar](http://search.maven.org/#artifactdetails%7Cnet.sf.opencsv%7Copencsv%7C2.3%7Cjar)
  * [porter-stemmer-1.4.jar](http://search.maven.org/#artifactdetails%7Cgov.sandia.foundry%7Cporter-stemmer%7C1.4%7Cjar)

```
org.apache.mahout:mahout-examples:jar:0.8:compile
+- org.apache.lucene:lucene-benchmark:jar:4.3.0:compile
|  +- org.apache.lucene:lucene-highlighter:jar:4.3.0:compile
|  |  \- org.apache.lucene:lucene-queries:jar:4.3.0:compile
|  +- org.apache.lucene:lucene-memory:jar:4.3.0:compile
|  +- org.apache.lucene:lucene-queryparser:jar:4.3.0:compile
|  |  \- org.apache.lucene:lucene-sandbox:jar:4.3.0:compile
|  |     \- jakarta-regexp:jakarta-regexp:jar:1.4:compile
|  +- org.apache.lucene:lucene-facet:jar:4.3.0:compile
|  +- com.ibm.icu:icu4j:jar:49.1:compile
|  +- net.sourceforge.nekohtml:nekohtml:jar:1.9.17:compile
|  +- org.apache.commons:commons-compress:jar:1.4.1:compile
|  \- xerces:xercesImpl:jar:2.9.1:compile
+- org.apache.lucene:lucene-analyzers-common:jar:4.3.0:compile
+- org.slf4j:slf4j-api:jar:1.7.5:compile
\- org.slf4j:slf4j-jcl:jar:1.7.5:runtime
   \- commons-logging:commons-logging:jar:1.1.1:compile
```
* spawn up Eclipse project: `mvn eclipse:eclipse -DdownloadSources=true`

##### References

* [Mahout on Amazon EMR: Elastic MapReduce](https://cwiki.apache.org/confluence/display/MAHOUT/Mahout+on+Elastic+MapReduce)
* [Yahoo Dev. Network - Hadoop Tutorial](http://developer.yahoo.com/hadoop/tutorial/)

![x](http://www.manning.com/holmes/holmes_cover150.jpg)
![x](http://www.manning.com/ingersoll/ingersoll_cover150.jpg)
![x](http://www.manning.com/owen/owen_cover150.jpg)

Good Bye!
