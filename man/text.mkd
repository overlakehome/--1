##### Keywords

* LDA topic-probability distribution per document (p(z|d))

##### Hadoop on Mac OS X - [ [Ubuntu](http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/) | [Mac OS X](http://wiki.apache.org/hadoop/Running_Hadoop_On_OS_X_10.5_64-bit_\(Single-Node_Cluster\)) ]

* `brew install hadoop`
* Setup [passphraseless ssh](http://hadoop.apache.org/docs/stable/single_node_setup.html#Setup+passphraseless)
  * `ssh-keygen -t rsa`, and then `cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys`
* Setup `libexec/conf/hadoop-env.sh` and [pseudo-distributed operation](http://hadoop.apache.org/docs/stable/single_node_setup.html#PseudoDistributed)
  * `curl -o '/usr/local/Cellar/hadoop/1.1.2/libexec/conf/hadoop-env.sh' \`
    `  -kL 'https://raw.github.com/henry4j/-/master/bin/hadoop-env.sh'`
  * `curl -o '/usr/local/Cellar/hadoop/1.1.2/libexec/conf/#1-site.xml' \`
    `  -kL 'https://raw.github.com/henry4j/-/master/bin/{core,hdfs,mapred}-site.xml'`
  * `rm -rf /usr/local/tmp/hadoop-$USER && mkdir -p /usr/local/tmp/hadoop-$USER`
  * `stop-all.sh && hadoop namenode -format`
* Bring up the [name node](http://localhost:50070/) and [job tracker](http://localhost:50030/)
* Upload & Downloads local files to DFS by [`hadoop dfs -put`](http://hadoop.apache.org/docs/stable/file_system_shell.html#put), [`hadoop dfs -get`](http://hadoop.apache.org/docs/stable/file_system_shell.html#get), and then [`hadoop dfs -ls`](http://hadoop.apache.org/docs/stable/file_system_shell.html#ls)

```bash
start-all.sh # starts up hadoop daemons: name, data, 2nd name nodes, and job & task tracker.
ps aux | grep hadoop | grep -o 'org.apache.[^ ]\+$' # sees 5 lines for hadoop daemons.
hadoop jar /usr/local/Cellar/hadoop/1.1.2/libexec/hadoop-examples-1.1.2.jar pi 10 100 # same as ruby -e 'p Math::PI'
stop-all.sh # stops hadoop daemons.
```

##### Mahout 0.7 w/ [0.8 patches](http://search.maven.org/#search%7Cga%7C1%7Cmahout)

* Source code as best documentation: `svn checkout http://svn.apache.org/repos/asf/mahout/trunk /workspace/mahout-trunk`

```bah
[ ! -d /workspace ] && sudo mkdir /workspace # make one unless there is.
curl -o $HOME/Downloads/mahout-distribution-0.7.zip \
  -kL http://www.globalish.com/am/mahout/0.7/mahout-distribution-0.7.zip
unzip -o $HOME/Downloads/mahout-distribution-0.7.zip -d /workspace/
ln -s /workspace/mahout-distribution-0.7 /workspace/mahout
```

```
mv /workspace/mahout/mahout-examples-0.7-job.jar /workspace/mahout/mahout-examples-0.7-job.jar.bak
mv /workspace/mahout/mahout-examples-0.7.jar /workspace/mahout/mahout-examples-0.7.jar.bak
curl -o /workspace/mahout/mahout-examples-0.8-job.jar \
  -kL http://search.maven.org/remotecontent?filepath=org/apache/mahout/mahout-examples/0.8/mahout-examples-0.8-job.jar
curl -o /workspace/mahout/mahout-examples-0.8.jar \
  -kL http://search.maven.org/remotecontent?filepath=org/apache/mahout/mahout-examples/0.8/mahout-examples-0.8.jar
```

##### Mahout LDA CVB Demo 


```bash
# Set up alias and work dir.
export MAHOUT="/workspace/mahout/bin/mahout" # same as mahout
export HADOOP="/usr/local/bin/hadoop" # same as hadoop
export WORK_DIR=/tmp/mahout-work-${USER}
[ ! -d $WORK_DIR ] && mkdir -p ${WORK_DIR}
```

###### Prepare Document Collection <sup>from Lucene Batchmark Program</sup> <sub>to Hadoop-native sequence files</sub>

```bash
if [ ! -e ${WORK_DIR}/reuters-ext ]; then
    if [ ! -e ${WORK_DIR}/reuters-sgm ]; then
        if [ ! -f $HOME/Downloads/reuters21578.tar.gz ]; then
            echo "Downloading Reuters-21578"
            curl http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz -o $HOME/Downloads/reuters21578.tar.gz
        fi
        mkdir -p ${WORK_DIR}/reuters-sgm
        echo "Extracting..."
        tar xzf $HOME/Downloads/reuters21578.tar.gz -C ${WORK_DIR}/reuters-sgm
    fi

    # Safe to ignore WARN: driver.MahoutDriver:
    #   [No org.apache.lucene.benchmark.utils.ExtractReuters.props found on classpath]
    #   (http://lucene.apache.org/core/4_4_0/benchmark/org/apache/lucene/benchmark/utils/ExtractReuters.html)
    echo "Extracting Reuters"
    $MAHOUT org.apache.lucene.benchmark.utils.ExtractReuters ${WORK_DIR}/reuters-sgm ${WORK_DIR}/reuters-ext

    echo "Copying Reuters data to Hadoop"
    $HADOOP dfs -rmr ${WORK_DIR}/reuters-ext || true
    $HADOOP dfs -put ${WORK_DIR}/reuters-ext ${WORK_DIR}/reuters-ext
    
    echo "Converting to Sequence Files"
    $MAHOUT seqdirectory -i ${WORK_DIR}/reuters-ext -o ${WORK_DIR}/reuters-seq -ow -c UTF-8 -chunk 5
fi
```

###### Topic Modeling w/o repetitive tuning cycles

* Let's do away with tuning required by seq2sparse and cvb for the better results.

```bash
$MAHOUT seq2sparse \
  -i ${WORK_DIR}/reuters-seq/ \
  -o ${WORK_DIR}/reuters-vector -ow --maxDFPercent 85 --namedVector # took 273,274 ms \
&& \
$MAHOUT rowid \
  -i ${WORK_DIR}/reuters-vector/tfidf-vectors \
  -o ${WORK_DIR}/reuters-matrix # took 5,209 ms \
&& \
rm -rf ${WORK_DIR}/reuters-lda ${WORK_DIR}/reuters-lda-topics ${WORK_DIR}/reuters-lda-model \
&& \
$MAHOUT cvb \
  -i ${WORK_DIR}/reuters-matrix/matrix \
  -dict ${WORK_DIR}/reuters-vector/dictionary.file-0 \
  -o ${WORK_DIR}/reuters-lda -ow -k 20 -x 20 \
  -dt ${WORK_DIR}/reuters-lda-topics \
  -mt ${WORK_DIR}/reuters-lda-model
```

```bash
13/07/26 12:17:51 INFO cvb.CVB0Driver: Completed 20 iterations in 2134 seconds
13/07/26 12:17:51 INFO cvb.CVB0Driver: Perplexities: ()
13/07/26 12:17:51 INFO cvb.CVB0Driver: About to run: Writing final topic/term distributions from /tmp/mahout-work-hylee/reuters-lda-model/model-20 to /tmp/mahout-work-hylee/reuters-lda
13/07/26 12:17:53 INFO cvb.CVB0Driver: About to run: Writing final document/topic inference from /tmp/mahout-work-hylee/reuters-matrix/matrix to /tmp/mahout-work-hylee/reuters-lda-topics
```

* `reuters-out-vector` by `seq2sparse`
  * tokenized-documents -- tokens (string tuples) keyed by docs.
     * `mahout seqdumper -i $WORK_DIR/reuters-vector/tokenized-documents -o token_vectors.txt`
     * e.g. key: /reut2-010.sgm-92.txt: value: [26, mar, 1987, 14, 39, 20.76, dexter, dex, units, sets, license, toyota, unit, dexter, corp's, hysol, aerospace, industrial, products, division, said, agreed, license, its, engineering, adhesives, toyota, motor, co's, toyoda, gosei, unit, two, units, jointly, develop, line, structural, adhesive, application, techniques, automotive, certain, other, industries, reuter, 3]
  * dictionary.file-0 -- integer IDs keyed by term.
     * `mahout seqdumper -i $WORK_DIR/reuters-vector/dictionary.file-0 -o dictionary.txt` # returns 41807 terms.
     * e.g. key: `0`: value: 0, key: `amazon`: value: 13707
  * wordcount
     * `mahout seqdumper -i $WORK_DIR/reuters-vector/wordcount -o wordcount.txt`
     * key: `amazon`: value: 6 # grep -w amazon token_vectors.txt | grep -o amazon | wc -l
  * frequency.file-0 -- frequencies keyed by term ID.
     * `mahout seqdumper -i $WORK_DIR/reuters-vector/frequency.file-0 -o frequency.txt`
     * e.g. key: 0: value: 43; key: 13707: value: 4 -- term `0` occurs 43 times; term `amazon` occurs in 4 documents.
  * df-count
     * `mahout seqdumper -i $WORK_DIR/reuters-vector/frequency.file-0 -q | grep 13707` # shows `amazon` occurs in 4 documents.
  * tf-vectors
     * `mahout vectordump -i $WORK_DIR/reuters-vector/tf-vectors/part-r-00000 -o tf-vectors.txt` # returns 21,578 docs.
     * `grep -o '13707:[[:digit:]]\+' tf-vectors.txt | paste -s -d, -` # 13707:1,13707:3,13707:1,13707:1
  * tfidf-vectors
     * `mahout vectordump -i $WORK_DIR/reuters-vector/tfidf-vectors -o tfidf_vectors.txt` # returns 21,578 docs.
     * `grep -o '13707:[[:digit:]]\+' tfidf_vectors.txt | paste -s -d, -` # 13707:9,13707:16,13707:9,13707:9
* reuters-matrix
  * docIndex
     * `mahout seqdumper -i $WORK_DIR/reuters-matrix/docIndex -q | head`
     * e.g. key: 1: value: /reut2-000.sgm-1.txt; key: 21577: value: /reut2-021.sgm-99.txt
  * matrix
     * `mahout seqdumper -i $WORK_DIR/reuters-matrix/matrix -o matrix.txt`
     * `grep -o '31707:[^,]\+' matrix.txt | paste -s -d, -` # see  
       31707:12.5864839553833,31707:8.899988174438477,31707:8.899988174438477,31707:8.899988174438477,  
       31707:15.415231704711914,31707:19.900978088378906,31707:15.415231704711914
* reuters-lda-topic -- final document/topic inference from $WORK_DIR/reuters-matrix/matrix
  * `mahout vectordump -i $WORK_DIR/reuters-lda-topics -o document-topic-inference.txt`
* reuters-lda-model -- final topic/term distributions from $WORK_DIR/reuters-lda-model/model-20
  * `mahout vectordump -i $WORK_DIR/reuters-lda-model/model-20 -o topic-term-distributions.txt \`  
    `-p true -sort $WORK_DIR/reuters-lda-model/model-20 -vs 15 \`  
    `-d $WORK_DIR/reuters-vector/dictionary.file-0 -dt sequencefile` # top 15 terms each topic.

```bash
ruby -e 'puts `head -n 2 document-topic-inference.txt`.split.map { |l| l = l[1..-2].split(","); l.reduce({}) { |h, d| (t,p) = d.split(":"); h.merge(t.to_i => (p.to_f * 100).to_i) } }'
{0=>30, 1=>3, 2=>2, 3=>22, 4=>0, 5=>0, 6=>0, 7=>22, 8=>0, 9=>0, 10=>13, 11=>0, 12=>2, 13=>0, 14=>0, 15=>0, 16=>0, 17=>0, 18=>0, 19=>1}
{0=>0, 1=>0, 2=>0, 3=>2, 4=>31, 5=>1, 6=>0, 7=>0, 8=>0, 9=>0, 10=>0, 11=>0, 12=>0, 13=>20, 14=>8, 15=>1, 16=>12, 17=>21, 18=>0, 19=>0}
```

```bash
head -n 2 topic-term-distributions.txt
0 {tonnes:5650.379357648988,ec:3606.4479551244567,export:2781.602557932994,european:2676.729832978985,trade:2548.7723083850838,sugar:2432.635346219812,community:2388.7343165287934,wheat:2270.674212672441,agriculture:2235.100329740205,u.s:2100.1009564052374,said:1987.6741323175618,countries:1667.656149141155,soviet:1658.0039337597923,exports:1640.0334612511813,imports:1439.8359812492943}
10	{crop:1614.0501397152202,corn:1328.097444980963,usda:1094.1952410131955,grain:1062.350145115043,said:1041.4191696874411,chrysler:1007.7248884961681,land:948.7121900578755,wheat:873.6906065406373,weather:871.1613470249432,crops:864.858131399484,mln:852.3232776187515,acres:815.7753911956302,areas:788.940463006496,gencorp:779.3432958209048,soybean:756.6295014149304}
```

###### LDA CBA - 2 Iterations

```bash
$MAHOUT cvb \
  -i ${WORK_DIR}/reuters-matrix/matrix \
  -o ${WORK_DIR}/reuters-lda-x2 -ow -k 20 -x 2 \
  -dict ${WORK_DIR}/reuters-vector/dictionary.file-* \
  -dt ${WORK_DIR}/reuters-lda-topics-x2 \
  -mt ${WORK_DIR}/reuters-lda-model-x2
```

```bash
13/07/23 16:40:03 INFO cvb.CVB0Driver: Completed 2 iterations in 787 seconds
13/07/23 16:40:03 INFO cvb.CVB0Driver: Perplexities: ()
13/07/23 16:40:03 INFO cvb.CVB0Driver: About to run: Writing final topic/term distributions from $WORK_DIR/reuters-lda-model-x2/model-2 to $WORK_DIR/reuters-lda-x2
13/07/23 16:40:04 INFO input.FileInputFormat: Total input paths to process : 10
13/07/23 16:40:05 INFO cvb.CVB0Driver: About to run: Writing final document/topic inference from $WORK_DIR/reuters-matrix/matrix to $WORK_DIR/reuters-lda-topics-x2
```

##### Make a doc-topic inference for a new doc by [TopicModel#trainDocTopicModel](https://builds.apache.org/job/Mahout-Quality/javadoc/org/apache/mahout/clustering/lda/cvb/TopicModel.html#trainDocTopicModel(org.apache.mahout.math.Vector, org.apache.mahout.math.Vector, org.apache.mahout.math.Matrix\))?

* [My Hello LDA! app -- to be updated w/ feedback from Jake M. (Principal SDE)](http://mahout.markmail.org/message/gjrfbjykwbjjm5gp)

```java
@Test
public void testDocumentTopicInferenceForNewDocsOverReuters() {
    val conf = new Configuration();
    val dictionary = readDictionary(new Path("/tmp/dictionary"), conf);
    assertThat(dictionary.length, equalTo(41807));

    // reads 'model' dense matrix (20 x 41K), and in 'topicSum' dense vector.
    TopicModel model = readModel(dictionary, new Path("/tmp/lda-model-splits"), conf);
    assertThat(model.getNumTopics(), equalTo(20));
    assertThat(model.getNumTerms(), equalTo(41807));

    val doc = takeOnlineDocument(conf);
    Vector docTopics = new DenseVector(new double[model.getNumTopics()]).assign(1.0/model.getNumTopics());
    Matrix docTopicModel = new SparseRowMatrix(model.getNumTopics(), doc.size());

    for (int i = 0; i < 25 /* maxItrs */; i++) {
        model.trainDocTopicModel(doc, docTopics, docTopicModel);
        System.out.println(docTopics.toString());
    }
}

private static Vector takeOnlineDocument(Configuration conf) {
    // tfidf_vector represents a document in RandomAccessSparseVector.
    // TODO: make a TFIDF vector out of an online string agaist dictionary, and frequency files.
    val tfidf_vector = readTFVectorsInRange(new Path("/tmp/tfidf-vectors"), conf, 0, 1)[0].getSecond();
    assertThat(tfidf_vector.size(), equalTo(41807));
    return tfidf_vector;
}

@SneakyThrows({ IOException.class })
private static Pair<String, Vector>[] readTFVectorsInRange(Path path, Configuration conf, int offset, int length) {
    val seq = new SequenceFile.Reader(FileSystem.get(conf), path, conf);
    val documentName = new Text();
    @SuppressWarnings("unchecked")
    Pair<String, Vector>[] vectors = new Pair[length];
    VectorWritable vector = new VectorWritable();
    for (int i = 0; i < offset + length && seq.next(documentName, vector); i++) {
        if (i >= offset) {
            vectors[i - offset] = Pair.of(documentName.toString(), vector.get());
        }
    }
    return vectors;
}

@SneakyThrows({ IOException.class })
private static TopicModel readModel(String[] dictionary, Path path, Configuration conf) {
    double alpha = 0.0001; // default: doc-topic smoothing
    double eta = 0.0001; // default: term-topic smoothing
    double modelWeight = 1f;
    return new TopicModel(conf, eta, alpha, dictionary, 1, modelWeight, listModelPath(path, conf));
}

@SneakyThrows({ IOException.class })
private static Path[] listModelPath(Path path, Configuration conf) {
    if (FileSystem.get(conf).isFile(path)) {
        return new Path[] { path };
    } else {
        val statuses = FileSystem.get(conf).listStatus(path, PathFilters.partFilter());
        val modelPaths = new Path[statuses.length];
        for (int i = 0; i < statuses.length; i++) {
            modelPaths[i] = new Path(statuses[i].getPath().toUri().toString());
        }
        return modelPaths;
    }
}

@SneakyThrows({ IOException.class })
private static String[] readDictionary(Path path, Configuration conf) {
    val term = new Text();
    val id = new IntWritable();
    val reader = new SequenceFile.Reader(FileSystem.get(conf), path, conf);
    val termIds = ImmutableList.<Pair<String, Integer>>builder();
    int maxId = 0;
    while (reader.next(term, id)) {
        termIds.add(Pair.of(term.toString(), id.get()));
        maxId = max(maxId, id.get());
    }
    String[] terms = new String[maxId + 1];
    for (val termId : termIds.build()) {
        terms[termId.getSecond().intValue()] = termId.getFirst().toString();
    }
    return terms;
}
```

##### Introduction to Clustering Text -- [Mahout algorithms](https://cwiki.apache.org/confluence/display/MAHOUT/Algorithms)

* text clustering: having a text processing tool that can automatically group similar items and present the results with summarizing labels is a good way to wade through large amount of text or search results without having to read all, or even most, of the content.
* cluster is an unsupervised task w/ no human interaction, such as annotating training text, required that can automatically put related content into buckets; in some cases, it also can assign **labels** to buckets and even give **summaries** of what's in each bucket.
* carrot vs. mahout
  * merits and demerits.
* how clustering can be appied at the word- or topic-level to identify keywords, or topics in documents (called topic modeling) using LDA (latent dirichlet allocation).
* performance: how fast? and how good?
* e.g. Google News, not just good algos, but **scalability**
  * factors: title, text, and publication time; they use various clustering algorithms to identify closely related stories.
  * there is more than running clustering algorithms - grouping news content on a near-real-time basis - designed to be scale.
  * quickly cluster large # of documents, determine representative documents or labels for display, and deal with new, incoming documents.

##### Types of clustering

* clustering is also useful for many other things besides text, like grouping users or data from a series of sensors, but those are outside the scope of this book.
* clustering documents is usually an offline batch processing job, so it's often worthwhile to spend extra time to get better results.
  * descriptions for clusters are often generated by looking at the most important terms by some weighting mechanism (such as TF-IDF) in documents closest to the centroid.
  * using n-grams (or identifying phrases) may also be worth experimenting with when testing description(?) approaches.
* clustering words into topics (also called topic modeling) is an effective way to quickly find topics that are covered in a large set of documents.
  * we assume that documents often cover several different topics and that words related to a given topic often found near each other -- quickly find which words appearch near each other and what documents are attached those words; in a sense, topic modeling also does document clustering.
  * topics themselves lack names, so naming it is the job of the person generating the topics. generating topics for a collection is one more way to aid users in browsing the collection and discovering interests, while we don't even know what documents contain which topic.

<table>
  <thead>
    <tr>
      <td>Topic 0</td>
      <td>Topic 1</td>
    </th>
  </thread>
  <tbody>
    <tr>
      <td>win saturday time game know nation u more after two take over back has from texa first day man offici 2 high one sinc some sunday</td>
      <td>yesterday game work new last over more most year than two from state after been would us polic peopl team run were open five american</td>
    </tr>
  </tbody>
</table>

* choosing clustering algorithms
  * hieararchical approach that runs non-linear time vs. flat approach that runs in linear time.
  * in hard- vs. soft-membership, required to rebuild clusters with new documents or not.
  * adaptive with user feedback, required to specify the number of clusters, performance and quality.
* similarity is implemented as a measure of distance between two documents that are represented as sparse vectors in p-norm with TF-IDF weights.
  * Euclidean or Cosine distance measures are appropriate for 2-norm, while Manhattan distance measure is for 1-norm.
* labeling clustering results involves utilizing concept/topic modeling by Latent Dirichlet Allocation, or
  * picking representative documents from a cluster (randomly, near from centroid, or by membership likelihood).
  * picking good labels by important terms or phrases in a cluster, a weighted list of terms by TF-IDF, a list of phrases by n-grams.

###### Install [Eclipse Kepler (4.3)](http://www.eclipse.org/downloads/)

```bash
curl -o ~/Downloads/eclipse-standard-kepler-R-macosx-cocoa-x86_64.tar.gz -kL http://ftp.osuosl.org/pub/eclipse/technology/epp/downloads/release/kepler/R/eclipse-standard-kepler-R-macosx-cocoa-x86_64.tar.gz
tar xvf ~/Downloads/eclipse-standard-kepler-R-macosx-cocoa-x86_64.tar.gz -C /Applications/
open /Applications/eclipse/Eclipse.app # and then keep this in dock
```

* Eclipse | Help | Install New Software... | Add...
* Enter `m2e` and `http://download.eclipse.org/technology/m2e/releases/` into Add Repository | OK
* Select All | Next | Next | Accept EULA | Finish | Restart Now

##### How-To: Spawn Up a text taming app Java project

```bash
echo | mvn archetype:generate \
  -DarchetypeGroupId=org.apache.maven.archetypes \
  -DarchetypeArtifactId=maven-archetype-quickstart \
  -DarchetypeVersion=1.1 \
  -DgroupId=com.henry4j \
  -DartifactId=text \
  -Dversion=1.0-SNAPSHOT \
  -DpackageName=com.henry4j \
  -DinteractiveMode=false
```

* add dependencies to pom.xml -- see [head revision](https://github.com/henry4j/-/blob/master/sources/text/pom.xml)
  * [google-guava-14.0.1.jar](http://search.maven.org/#artifactdetails%7Ccom.google.guava%7Cguava%7C14.0.1%7Cbundle)
  * [opencsv-2.3.jar](http://search.maven.org/#artifactdetails%7Cnet.sf.opencsv%7Copencsv%7C2.3%7Cjar)
  * [porter-stemmer-1.4.jar](http://search.maven.org/#artifactdetails%7Cgov.sandia.foundry%7Cporter-stemmer%7C1.4%7Cjar)

```
org.apache.mahout:mahout-examples:jar:0.8:compile
+- org.apache.lucene:lucene-benchmark:jar:4.3.0:compile
|  +- org.apache.lucene:lucene-highlighter:jar:4.3.0:compile
|  |  \- org.apache.lucene:lucene-queries:jar:4.3.0:compile
|  +- org.apache.lucene:lucene-memory:jar:4.3.0:compile
|  +- org.apache.lucene:lucene-queryparser:jar:4.3.0:compile
|  |  \- org.apache.lucene:lucene-sandbox:jar:4.3.0:compile
|  |     \- jakarta-regexp:jakarta-regexp:jar:1.4:compile
|  +- org.apache.lucene:lucene-facet:jar:4.3.0:compile
|  +- com.ibm.icu:icu4j:jar:49.1:compile
|  +- net.sourceforge.nekohtml:nekohtml:jar:1.9.17:compile
|  +- org.apache.commons:commons-compress:jar:1.4.1:compile
|  \- xerces:xercesImpl:jar:2.9.1:compile
+- org.apache.lucene:lucene-analyzers-common:jar:4.3.0:compile
+- org.slf4j:slf4j-api:jar:1.7.5:compile
\- org.slf4j:slf4j-jcl:jar:1.7.5:runtime
   \- commons-logging:commons-logging:jar:1.1.1:compile
```
* spawn up Eclipse project: `mvn eclipse:eclipse -DdownloadSources=true`

##### References

* [Mahout on Amazon EMR: Elastic MapReduce](https://cwiki.apache.org/confluence/display/MAHOUT/Mahout+on+Elastic+MapReduce)
* [Yahoo Dev. Network - Hadoop Tutorial](http://developer.yahoo.com/hadoop/tutorial/)

![x](http://www.manning.com/holmes/holmes_cover150.jpg)
![x](http://www.manning.com/ingersoll/ingersoll_cover150.jpg)
![x](http://www.manning.com/owen/owen_cover150.jpg)

Good Bye!
