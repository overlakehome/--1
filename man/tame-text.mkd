##### Analyzer Basics

Solr applies an analysis process to fields being indexed to stem words, remove stopwords, and otherwise alter the tokens to be indexed. The Lucene Analyzer class controls this process, and consists of an optional **CharFilter**, a required **Tokenizer**, and zero or more **TokenFilter**s.  
* A `CharFilter` can be used to remove content while maintaining correct offset information (such as stripping HTML tags) for things like highlighting. In most cases, you won’t need a `CharFilter`.
* A `Tokenizer` produces Tokens, which in most cases correspond to words to be indexed.
* A `TokenFilter` then takes Tokens from the `Tokenizer` and optionally modifies or removes the Tokens before giving them back to Lucene for indexing.
* e.g., Solr's `WhitespaceTokenizer` breaks words on whitespace, and its `StopFilter` removes common words from search results.

```xml
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<!-- good for query analyzer, not for index analyzer -->
<!-- <filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/> -->
<filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt"/>
<filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0" splitOnCaseChange="1"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.EnglishPorterFilterFactory" protected="protwords.txt"/>
<filter class="solr.RemoveDuplicatesTokenFilterFactory"/>

<!-- good for fuzzy string matching -->
<!-- <tokenizer class="solr.KeywordTokenizerFactory"/> -->
<!-- <tokenizer class="solr.PatternTokenizerFactory" pattern="." group="0" /> -->
<!-- <filter class="solr.EdgeNGramFilterFactory" side="front" minGramSize="2" maxGramSize="3"/> -->
```

##### Analyzers

* MailArchivesClusteringAnalyzer (more aggresive than the default StandardAnalyzer) uses a broader set of stopwords, excludes nonalphanumeric tokens, and applies porter stemming.
* WhitespaceAnalyzer performs simpletokenization of the input data. The data will have stemming performed and stopwords removed using Lucene’s EnglishAnalyzer later as a part of the training and test process, so there’s no need to perform anything other than whitespace tokenization at this point. Other classifiers such as Mahout's Bayes classifier benefit from performing stemming and stopword removal as a part of the data preparation phase.

```java
Directory directory = FSDirectory.open(new File(pathname));
Analyzer analyzer = new EnglishAnalyzer(Version.LUCENE_36);
if (nGramSize > 1) {
    ShingleAnalyzerWrapper sw = new ShingleAnalyzerWrapper(analyzer,
            nGramSize, // min shingle size
            nGramSize, // max shingle size
            "-", // token separator
            true, // output unigrams
            true); // output unigrams if no shingles
    analyzer = sw;
}
```

#
* [o.a.l.analysis.coreStopAnalyzer](http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/StopAnalyzer.java?view=markup)
* [o.a.l.analysis.core.WhitespaceAnalyzer extends Analyzer](http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceAnalyzer.java?view=markup)
* [o.a.l.analysis.en.EnglishAnalyzer extends StopwordAnalyzerBase](http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java?view=markup)

* [o.a.l.analysis.standard.StandardAnalyzer extends StopwordAnalyzerBase](http://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/standard/StandardAnalyzer.html)
  -- [svn](http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java?view=markup)
  * filters StandardTokenizer with StandardFilter, LowerCaseFilter and StopFilter, using a list of English stop words.
  * standard tokenizer w/ 255 max token length, filters: lower-case, and stop.
  * 33 stop-words: ["a", "an", "and", "are", "as", "at", "be", "but", "by", "for", "if", "in", "into", "is", "it", "no", "not", "of", "on", "or", "such", "that", "the", "their", "then", "there", "these", "they", "this", "to", "was", "will", "with"]
* [o.a.m.text.MailArchivesClusteringAnalyzer extends StopwordAnalyzerBase](http://www.java2s.com/Open-Source/Java-Open-Source-Library/Data-Mnining/mahout/org/apache/mahout/text/MailArchivesClusteringAnalyzer.java.java-doc.htm)
  -- [svn](http://svn.apache.org/viewvc/mahout/trunk/integration/src/main/java/org/apache/mahout/text/MailArchivesClusteringAnalyzer.java?view=markup)
  * custom Lucene Analyzer designed for aggressive feature reduction for clustering the ASF Mail Archives using an extended set of stop words, excluding non-alpha-numeric tokens, and porter stemming.
  * standard tokenizer, filters: lower-case, ascii-folding, alpha-numeric (2 - 40 chars long), stop, and porter-stem.
  * 471 stop-words w/o "a".
* [o.a.l.analysis.standard.UAX29URLEmailAnalyzer extends StopwordAnalyzerBase](http://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/standard/UAX29URLEmailAnalyzer.html)
  -- [svn](http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailAnalyzer.java?view=markup)
  * filters UAX29URLEmailTokenizer with StandardFilter, LowerCaseFilter and StopFilter, using a list of English stop words.
  * [o.a.l.analysis.standard.UAX29URLEmailTokenizer extends Tokenizer](https://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.html)

#
* [o.a.m.text.wikipedia.WikipediaAnalyzer extends StopwordAnalyzerBase](http://svn.apache.org/viewvc/mahout/trunk/integration/src/main/java/org/apache/mahout/text/wikipedia/WikipediaAnalyzer.java?view=markup)
* [org.dspace.search.DSAnalyzer](https://svn.duraspace.org/view/dspace/dspace/trunk/dspace-api/src/main/java/org/dspace/search/DSAnalyzer.java?view=markup)
* [org.yooreeka.algos.search.lucene.analyzer.CustomAnalyzer](http://yooreeka.googlecode.com/svn-history/r87/trunk/src/org/yooreeka/algos/search/lucene/analyzer/CustomAnalyzer.java)
* [org.icatproject.core.manager.IcatAnalyzer](https://code.google.com/p/icatproject/source/browse/icat/trunk/core/src/main/java/org/icatproject/core/manager/IcatAnalyzer.java?r=2511)
* [org.icatproject.core.manager.ESNAnalyzer](https://code.google.com/p/icatproject/source/browse/icat/trunk/core/src/main/java/org/icatproject/core/manager/ESNAnalyzer.java?spec=svn2499&r=2499)

#
* http://svn.apache.org/viewvc/mahout/trunk/core/src/main/java/org/apache/mahout/common/lucene/
  * [AnalyzerUtils#createAnalzyer](http://svn.apache.org/viewvc/mahout/trunk/core/src/main/java/org/apache/mahout/common/lucene/AnalyzerUtils.java?view=markup)
* [org.apache.mahout.utils.regex.AnalyzerTransformer implements RegexTransformer](http://svn.apache.org/viewvc/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/regex/AnalyzerTransformer.java?view=markup)

##### mahout seq2sparse --help

* -s 500 (default 2, also called --minSupport) -- excludes that don't occur 500- times across all documents.
* -x 70 (default 99, also called --maxDFPercent) -- excludes that occur in 70+% documents.
* -ng 1 (default 1, also called --maxNGramSize) -- 2 for bigram, 3 for trigram, etc.
* -a org.apache.mahout.text.MailArchivesClusteringAnalyzer
* -a org.apache.lucene.analysis.WhitespaceAnalyzer

###### (maybe)

* -md 1 (default: 1, also called --minDF)
* -xs -1 (default: -1, also called --maxDFSigma) -- a good value to be 3.0.
* -wt (default: tfidf, also called --weight) -- tf or tfidf.
* -n (default: -1, also called --norm)
* -ml (1.0, also called --minLLR) - minimum log likelihood ratio.
* -lnorm (default: false, also called --logNormalize)
