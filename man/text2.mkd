#### Keywords

* http://blog.swwomm.com/2013/07/tuning-lucene-to-get-most-relevant.html
* http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop

#### TFIDF

```bash
mahout seq2sparse --weight TF, or TFIDF
```

* SparseVectorsFromSequenceFiles.java -- processIdf = true by default.

```java
DictionaryVectorizer.createTermFrequencyVectors(tokenizedPath, // "./tokenized-documents"
        outputDir, // ${WORK_DIR}/comm-text-bigram.
        tfDirName, // "./tf-vectors" + "-topprune".
        conf, // getConf() for a hadoop conf.
        minSupport,
        maxNGramSize,
        minLLRValue,
        -1.0f /* NO_NORMALIZING for normPower */,
        false,
        reduceTasks,
        chunkSize,
        sequentialAccessOutput,
        namedVectors);
        
public static void createTermFrequencyVectors(...) {
    startWordCounting(input, dictionaryJobPath, baseConf, minSupport);
}
```

* TermCountMapper: 
* TermCountCombiner
* TermCountReducer

##### CVB params

* 8K documents, 
* typically for English, reasonable topics b/w 20 and 200 (tending toward the lower-end unless there are very many documents).
  * 20 topics'll yield very generic things, 100 is pretty nice, a lot of the time, but 200+ can lead to really niche things.
* 20 - 30 iterations tend to be always be enough, but check for perplexity; doc-topic distribution's plateaued, as perplexity's.
  * In practice, Jake M. never needed more than 30 iterations; less the larger the corpus is.
* for smoothing doc-topic and topic-term distribution, do a grid search over (α, β) = {0.001, 0.01, 0.1} x {0.001, 0.01, 0.1}.

##### Analyzer Basics

Solr applies an analysis process to fields being indexed to stem words, remove stopwords, and otherwise alter the tokens to be indexed. The Lucene Analyzer class controls this process, and consists of an optional **CharFilter**, a required **Tokenizer**, and zero or more **TokenFilter**s.  
* A `CharFilter` can be used to remove content while maintaining correct offset information (such as stripping HTML tags) for things like highlighting. In most cases, you won’t need a `CharFilter`.
* A `Tokenizer` produces Tokens, which in most cases correspond to words to be indexed.
* A `TokenFilter` then takes Tokens from the `Tokenizer` and optionally modifies or removes the Tokens before giving them back to Lucene for indexing.
* e.g., Solr's `WhitespaceTokenizer` breaks words on whitespace, and its `StopFilter` removes common words from search results.

```xml
<tokenizer class="solr.WhitespaceTokenizerFactory"/>
<!-- good for query analyzer, not for index analyzer -->
<!-- <filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/> -->
<filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt"/>
<filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0" splitOnCaseChange="1"/>
<filter class="solr.LowerCaseFilterFactory"/>
<filter class="solr.EnglishPorterFilterFactory" protected="protwords.txt"/>
<filter class="solr.RemoveDuplicatesTokenFilterFactory"/>

<!-- good for fuzzy string matching -->
<!-- <tokenizer class="solr.KeywordTokenizerFactory"/> -->
<!-- <tokenizer class="solr.PatternTokenizerFactory" pattern="." group="0" /> -->
<!-- <filter class="solr.EdgeNGramFilterFactory" side="front" minGramSize="2" maxGramSize="3"/> -->
```

##### Analyzers

* MailArchivesClusteringAnalyzer (more aggresive than the default StandardAnalyzer) uses a broader set of stopwords, excludes nonalphanumeric tokens, and applies porter stemming.
* WhitespaceAnalyzer performs simpletokenization of the input data. The data will have stemming performed and stopwords removed using Lucene’s EnglishAnalyzer later as a part of the training and test process, so there’s no need to perform anything other than whitespace tokenization at this point. Other classifiers such as Mahout's Bayes classifier benefit from performing stemming and stopword removal as a part of the data preparation phase.

```java
Directory directory = FSDirectory.open(new File(pathname));
Analyzer analyzer = new EnglishAnalyzer(Version.LUCENE_36);
if (nGramSize > 1) {
    ShingleAnalyzerWrapper sw = new ShingleAnalyzerWrapper(analyzer,
            nGramSize, // min shingle size
            nGramSize, // max shingle size
            "-", // token separator
            true, // output unigrams
            true); // output unigrams if no shingles
    analyzer = sw;
}
```

#
* [o.a.l.analysis.coreStopAnalyzer](http://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/core/StopAnalyzer.html)
  -- [svn](http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/StopAnalyzer.java?view=markup)
  * filters LetterTokenizer with LowerCaseFilter and StopFilter.
* [o.a.l.analysis.core.WhitespaceAnalyzer extends Analyzer](http://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/core/WhitespaceAnalyzer.html)
  -- [svn](http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceAnalyzer.java?view=markup)
  * uses [o.a.l.analysis.core.WhitespaceTokenizer](http://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/core/WhitespaceTokenizer.html)
* [o.a.l.analysis.en.EnglishAnalyzer extends StopwordAnalyzerBase](http://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/en/EnglishAnalyzer.html)
  -- [svn](http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java?view=markup)
  * filters StandardTokenizer with StandardFilter, EnglishPossessiveFilter, PorterStemFilter w/ a stem exclusion list, LowerCaseFilter and StopFilter, using a list of English stop words.
  * uses [o.a.l.analysis.en.EnglishPossessiveFilter for trailing `'s`](http://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/en/EnglishPossessiveFilter.html)
* [o.a.l.analysis.standard.StandardAnalyzer extends StopwordAnalyzerBase](http://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/standard/StandardAnalyzer.html)
  -- [svn](http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java?view=markup)
  * filters StandardTokenizer with StandardFilter, LowerCaseFilter and StopFilter, using a list of English stop words.
  * uses [o.a.l.analysis.standard.StandardTokenizer](http://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/standard/StandardTokenizer.html)
     * A grammar-based tokenizer constructed with JFlex.
     * As of Lucene version 3.1, this class implements the Word Break rules from the Unicode Text Segmentation algorithm, as specified in Unicode Standard Annex #29.
     * Many applications have specific tokenizer needs. If this tokenizer does not suit your application, please consider copying this source code directory to your project and maintaining your own grammar-based tokenizer.
  * standard tokenizer w/ 255 max token length, filters: lower-case, and stop.
  * 33 stop-words: ["a", "an", "and", "are", "as", "at", "be", "but", "by", "for", "if", "in", "into", "is", "it", "no", "not", "of", "on", "or", "such", "that", "the", "their", "then", "there", "these", "they", "this", "to", "was", "will", "with"]
* [o.a.m.text.MailArchivesClusteringAnalyzer extends StopwordAnalyzerBase](http://www.java2s.com/Open-Source/Java-Open-Source-Library/Data-Mnining/mahout/org/apache/mahout/text/MailArchivesClusteringAnalyzer.java.java-doc.htm)
  -- [svn](http://svn.apache.org/viewvc/mahout/trunk/integration/src/main/java/org/apache/mahout/text/MailArchivesClusteringAnalyzer.java?view=markup)
  * custom Lucene Analyzer designed for aggressive feature reduction for clustering the ASF Mail Archives using an extended set of stop words, excluding non-alpha-numeric tokens, and porter stemming.
  * standard tokenizer, filters: lower-case, ascii-folding, alpha-numeric (2 - 40 chars long), stop, and porter-stem.
  * 471 stop-words w/o "a".
* [o.a.l.analysis.standard.UAX29URLEmailAnalyzer extends StopwordAnalyzerBase](http://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/standard/UAX29URLEmailAnalyzer.html)
  -- [svn](http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailAnalyzer.java?view=markup)
  * filters UAX29URLEmailTokenizer with StandardFilter, LowerCaseFilter and StopFilter, using a list of English stop words.
  * uses [o.a.l.analysis.standard.UAX29URLEmailTokenizer extends Tokenizer](https://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.html)
* [o.a.m.text.wikipedia.WikipediaAnalyzer extends StopwordAnalyzerBase](http://svn.apache.org/viewvc/mahout/trunk/integration/src/main/java/org/apache/mahout/text/wikipedia/WikipediaAnalyzer.java?view=markup)
  * [o.a.l.analysis.wikipedia.WikipediaTokenizer](http://lucene.apache.org/core/3_6_2/api/all/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.html)
     * Extension of StandardTokenizer that is aware of Wikipedia syntax. It is based off of the Wikipedia tutorial available at http://en.wikipedia.org/wiki/Wikipedia:Tutorial, but it may not be complete.

#
* [org.dspace.search.DSAnalyzer](https://svn.duraspace.org/view/dspace/dspace/trunk/dspace-api/src/main/java/org/dspace/search/DSAnalyzer.java?view=markup)
* [org.yooreeka.algos.search.lucene.analyzer.CustomAnalyzer](http://yooreeka.googlecode.com/svn-history/r87/trunk/src/org/yooreeka/algos/search/lucene/analyzer/CustomAnalyzer.java)
* [org.icatproject.core.manager.IcatAnalyzer](https://code.google.com/p/icatproject/source/browse/icat/trunk/core/src/main/java/org/icatproject/core/manager/IcatAnalyzer.java?r=2511)
* [org.icatproject.core.manager.ESNAnalyzer](https://code.google.com/p/icatproject/source/browse/icat/trunk/core/src/main/java/org/icatproject/core/manager/ESNAnalyzer.java?spec=svn2499&r=2499)
* org.apache.lucene.analysis.gosen.GosenAnalyzer
* org.geotoolkit.lucene.analysis.standard.ClassicAnalyzer
* org.projectforge.lucene.StandardAnalyzer
* org.elasticsearch.index.analysis.StandardHtmlStripAnalyzer
* org.openedit.data.lucene.FullTextAnalyzer
* nc.ird.cantharella.data.model.search.CantharellaAnalyzer
* eu.dicodeproject.analysis.lucene.TweetAnalyzer
* org.greenstone.LuceneWrapper3.GS2StandardAnalyzer
* org.apache.lucene.analysis.ca.CatalanAnalyzer

#
* http://svn.apache.org/viewvc/mahout/trunk/core/src/main/java/org/apache/mahout/common/lucene/
  * [AnalyzerUtils#createAnalzyer](http://svn.apache.org/viewvc/mahout/trunk/core/src/main/java/org/apache/mahout/common/lucene/AnalyzerUtils.java?view=markup)
* [org.apache.mahout.utils.regex.AnalyzerTransformer implements RegexTransformer](http://svn.apache.org/viewvc/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/regex/AnalyzerTransformer.java?view=markup)

##### mahout seq2sparse --help

* -s 500 (default 2, also called --minSupport) -- excludes that don't occur 500- times across all documents.
* -x 70 (default 99, also called --maxDFPercent) -- excludes that occur in 70+% documents.
* -ng 1 (default 1, also called --maxNGramSize) -- 2 for bigram, 3 for trigram, etc.
* -a org.apache.mahout.text.MailArchivesClusteringAnalyzer
* -a org.apache.lucene.analysis.WhitespaceAnalyzer

###### (maybe)

* -md 1 (default: 1, also called --minDF)
* -xs -1 (default: -1, also called --maxDFSigma) -- a good value to be 3.0.
* -wt (default: tfidf, also called --weight) -- tf or tfidf.
* -n (default: -1, also called --norm)
* -ml (1.0, also called --minLLR) - minimum log likelihood ratio.
* -lnorm (default: false, also called --logNormalize)
